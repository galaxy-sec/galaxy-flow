#[cfg(test)]
mod tests {
    use crate::ai::{
        capabilities::AiDevCapability,
        client::AiClient,
        provider::{AiProvider, AiProviderType, AiRequestBuilder},
        providers::{mock::MockProvider, openai::OpenAiProvider},
        AiConfig, AiError,
    };

    use std::error::Error;

    // ğŸ¯ å®é™…å¯ç”¨çš„æµ‹è¯•ç”¨ä¾‹

    #[tokio::test]
    async fn test_mock_provider_workflow() -> Result<(), Box<dyn Error>> {
        println!("ğŸ”§ Testing Mock Provider...");

        let provider = MockProvider::new();

        let request = AiRequestBuilder::new()
            .model("mock-model")
            .system_prompt("ä½ æ˜¯ä¸€ä¸ªRustå·¥ç¨‹å¸ˆ")
            .user_prompt("ç”¨ä¸€å¥è¯è§£é‡Šæ‰€æœ‰æƒæœºåˆ¶")
            .capability(AiDevCapability::Explain)
            .max_tokens(50)
            .build();

        let response = provider.send_request(&request).await?;

        assert!(!response.content.is_empty());
        println!("âœ… Mock Response: {}", response.content);
        Ok(())
    }

    #[tokio::test]
    async fn test_openai_integration_if_available() -> Result<(), Box<dyn Error>> {
        println!("ğŸ” Testing OpenAI Integration...");

        if let Ok(api_key) = std::env::var("OPENAI_API_KEY") {
            let provider = OpenAiProvider::new(api_key);

            let request = AiRequestBuilder::new()
                .model("gpt-4o-mini")
                .system_prompt("ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹")
                .user_prompt("ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯é›¶ä¾èµ–")
                .capability(AiDevCapability::Suggest)
                .max_tokens(75)
                .temperature(0.7)
                .build();

            let response = provider.send_request(&request).await?;

            assert!(!response.content.is_empty());
            println!("ğŸŸ¢ OpenAI Response: {}", response.content);
        } else {
            println!("âš ï¸ Skipping OpenAI - no API key");
        }

        Ok(())
    }

    #[tokio::test]
    async fn test_deepseek_provider_instantiation() -> Result<(), Box<dyn Error>> {
        println!("ğŸ”§ Testing DeepSeek Provider Instantiation...");

        if let Ok(api_key) = std::env::var("DEEPSEEK_API_KEY") {
            let provider = OpenAiProvider::deep_seek(api_key.clone());

            // éªŒè¯Providerç±»å‹
            assert_eq!(provider.provider_type(), AiProviderType::DeepSeek);

            // éªŒè¯é…ç½®é”®
            let config_keys = provider.get_config_keys();
            assert!(config_keys.contains(&"DEEPSEEK_API_KEY"));
            assert!(config_keys.contains(&"DEEPSEEK_BASE_URL"));

            // éªŒè¯æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥
            assert!(provider.is_model_available("deepseek-chat").await);
            assert!(provider.is_model_available("deepseek-coder").await);
            assert!(!provider.is_model_available("gpt-4o").await); // éDeepSeekæ¨¡å‹

            println!("âœ… DeepSeek Provider instantiation successful");
            println!("ğŸ“ Config keys: {:?}", config_keys);
        } else {
            println!("âš ï¸ Skipping DeepSeek provider tests - no DEEPSEEK_API_KEY");
        }

        Ok(())
    }

    #[tokio::test]
    async fn test_deepseek_model_list() -> Result<(), Box<dyn Error>> {
        println!("ğŸ“‹ Testing DeepSeek Model List...");

        if let Ok(api_key) = std::env::var("DEEPSEEK_API_KEY") {
            let provider = OpenAiProvider::deep_seek(api_key);

            // éªŒè¯æ¨¡å‹åˆ—è¡¨
            let models = provider.list_models().await?;
            assert!(!models.is_empty());

            // éªŒè¯DeepSeekæ¨¡å‹å­˜åœ¨
            let model_names: Vec<&str> = models.iter().map(|m| m.name.as_str()).collect();
            assert!(model_names.contains(&"deepseek-chat"));
            assert!(model_names.contains(&"deepseek-coder"));
            assert!(model_names.contains(&"deepseek-reasoner"));

            // éªŒè¯æ¨¡å‹ä¿¡æ¯
            for model in models {
                assert_eq!(model.provider, AiProviderType::DeepSeek);
                assert!(model.max_tokens > 0);
                assert!(model.cost_per_1k_input >= 0.0);
                assert!(model.cost_per_1k_output >= 0.0);

                println!(
                    "ğŸ“Š Model: {}, Max tokens: {}, Cost: ${}/{}/1k tokens",
                    model.name, model.max_tokens, model.cost_per_1k_input, model.cost_per_1k_output
                );
            }

            println!("âœ… DeepSeek model list verification successful");
        } else {
            println!("âš ï¸ Skipping DeepSeek model list tests - no DEEPSEEK_API_KEY");
        }

        Ok(())
    }

    #[tokio::test]
    async fn test_ai_client_e2e() -> Result<(), Box<dyn Error>> {
        println!("ğŸ¯ Testing AI Client End-to-End...");

        let config = AiConfig::load()?;
        let client = AiClient::new(config)?;

        // Test 1: Basic request
        let response = client
            .smart_request(
                AiDevCapability::Generate,
                "å†™ä¸€ä¸ªç®€å•çš„Hello World Rustç¨‹åº",
            )
            .await?;

        assert!(!response.content.is_empty());
        println!("ğŸ³ Generated: {}", response.content.trim());

        // Test 2: Smart commit
        let commit = client
            .smart_commit("ä¿®å¤äº†è¾“å…¥éªŒè¯é€»è¾‘ï¼Œæ·»åŠ äº†è¾¹ç•Œæ£€æŸ¥")
            .await?;

        assert!(!commit.content.is_empty());
        assert!(commit.content.len() <= 75);
        println!("ğŸ“ Smart commit: {}", commit.content);

        // Test 3: Code review
        let review = client
            .code_review("fn divide(a:i32, b:i32) -> i32 { a / b }", "math_utils.rs")
            .await?;

        assert!(!review.content.is_empty());
        println!("ğŸ” Code review: {}", review.content);

        Ok(())
    }

    #[test]
    fn test_module_compilation() {
        // Basic compilation test
        assert!(true);
    }

    #[test]
    fn test_capability_models() {
        assert_eq!(AiDevCapability::Analyze.recommended_model(), "gpt-4o-mini");
        assert_eq!(AiDevCapability::Generate.recommended_model(), "gpt-4o");
        assert_eq!(
            AiDevCapability::Review.recommended_model(),
            "claude-3-5-sonnet"
        );
        assert_eq!(AiDevCapability::Commit.recommended_model(), "gpt-4o-mini");
    }

    #[tokio::test]
    async fn test_deepseek_basic_api_call() -> Result<(), Box<dyn Error>> {
        println!("ğŸŒ Testing DeepSeek Basic API Call...");

        if let Ok(api_key) = std::env::var("DEEPSEEK_API_KEY") {
            let provider = OpenAiProvider::deep_seek(api_key);

            let request = AiRequestBuilder::new()
                .model("deepseek-chat")
                .system_prompt("ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹")
                .user_prompt("ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯é›¶ä¾èµ–è®¾è®¡")
                .capability(AiDevCapability::Suggest)
                .max_tokens(100)
                .temperature(0.7)
                .build();

            let response = provider.send_request(&request).await?;

            // éªŒè¯åŸºæœ¬å“åº”
            assert!(!response.content.is_empty());
            assert_eq!(response.provider, AiProviderType::DeepSeek);
            assert!(response.model.contains("deepseek"));

            // éªŒè¯ä½¿ç”¨ç»Ÿè®¡
            assert!(response.usage.prompt_tokens > 0);
            assert!(response.usage.completion_tokens > 0);
            assert!(response.usage.total_tokens > 0);
            assert!(response.usage.estimated_cost.is_some());

            println!("ğŸŸ¢ DeepSeek API call successful");
            println!("ğŸ’¬ Response: {}", response.content.trim());
            println!(
                "ğŸ“Š Usage: {} input + {} output = {} tokens",
                response.usage.prompt_tokens,
                response.usage.completion_tokens,
                response.usage.total_tokens
            );
            println!(
                "ğŸ’° Cost: ${:.6}",
                response.usage.estimated_cost.unwrap_or(0.0)
            );
        } else {
            println!("âš ï¸ Skipping DeepSeek API call tests - no DEEPSEEK_API_KEY");
        }

        Ok(())
    }

    #[tokio::test]
    async fn test_deepseek_error_handling() -> Result<(), Box<dyn Error>> {
        println!("ğŸš¨ Testing DeepSeek Error Handling...");

        if let Ok(api_key) = std::env::var("DEEPSEEK_API_KEY") {
            // æµ‹è¯•æ— æ•ˆæ¨¡å‹é”™è¯¯
            let provider = OpenAiProvider::deep_seek(api_key.clone());

            let invalid_request = AiRequestBuilder::new()
                .model("invalid-model-name")
                .system_prompt("æµ‹è¯•")
                .user_prompt("æµ‹è¯•")
                .build();

            let result = provider.send_request(&invalid_request).await;
            assert!(result.is_err());

            match result {
                Err(AiError::NetworkError(_))
                | Err(AiError::AuthError(_))
                | Err(AiError::InvalidModel(_)) => {
                    println!("âœ… Expected error caught for invalid model");
                }
                Err(other_error) => {
                    panic!(
                        "Expected network/auth/model error, but got: {:?}",
                        other_error
                    );
                }
                Ok(_) => {
                    panic!("Expected error for invalid model, but got success");
                }
            }

            // æµ‹è¯•æ— æ•ˆAPIå¯†é’¥ï¼ˆä½¿ç”¨æ— æ•ˆå¯†é’¥ï¼‰
            let invalid_provider = OpenAiProvider::deep_seek("invalid-api-key".to_string());
            let valid_request = AiRequestBuilder::new()
                .model("deepseek-chat")
                .system_prompt("æµ‹è¯•")
                .user_prompt("æµ‹è¯•")
                .build();

            let invalid_result = invalid_provider.send_request(&valid_request).await;
            assert!(invalid_result.is_err());

            println!("âœ… DeepSeek error handling verification successful");
        } else {
            println!("âš ï¸ Skipping DeepSeek error handling tests - no DEEPSEEK_API_KEY");
        }

        Ok(())
    }

    #[tokio::test]
    async fn test_zero_config_mode() -> Result<(), Box<dyn Error>> {
        // Test the core functionality works even in "zero-config" mode
        let config = AiConfig::default();
        let client = AiClient::new(config)?;

        let providers = client.available_providers();
        assert!(!providers.is_empty());

        println!("âœ… Available providers: {:?}", providers);
        Ok(())
    }

    // ğŸš€ å®é™…è¿è¡Œå‘½ä»¤ï¼š
    // cargo test --test ai_tests -- --nocapture
    // OPENAI_API_KEY=your_key cargo test -- --test-threads=1
    // DEEPSEEK_API_KEY=your_key cargo test deepseek -- --nocapture
    // ./test_openai.sh
}
