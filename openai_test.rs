use galaxy_flow::ai::config::AiConfig;
use galaxy_flow::ai::{AiCapability, AiClient};
use std::error::Error;

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    println!("ğŸš€ GXL AI-Native Runtime Test");
    println!("=================================\n");

    // æ£€æŸ¥ API key
    let api_key = std::env::var("OPENAI_API_KEY").ok();
    if api_key.is_none() {
        println!("âš ï¸  è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY=your_key");
        println!("   æˆ–ä½¿ç”¨æœ¬åœ° AI: OLLAMA_MODEL=deepseek-coder");

        // åˆ›å»ºé…ç½®ï¼ˆä¼šå°è¯•ä»ç¯å¢ƒåŠ è½½ï¼‰
        let config = AiConfig::load()?;
        let client = AiClient::new(config)?;

        // ä½¿ç”¨ Mock æµ‹è¯•
        let response = client
            .smart_request(AiCapability::Commit, "æ·»åŠ äº†ç”¨æˆ·è®¤è¯åŠŸèƒ½")
            .await?;

        println!("ğŸ”§ Mock Response: {}", response.content);
        return Ok(());
    }

    println!("âœ… æ£€æµ‹åˆ° OpenAI API å¯†é’¥");

    // é…ç½®å’Œåˆå§‹åŒ–
    let config = AiConfig::load()?;
    let client = AiClient::new(config)?;

    // æµ‹è¯•1: æ™ºèƒ½ä»£ç ç†è§£
    println!("ğŸ“Š æµ‹è¯•1: ä»£ç åˆ†æ");
    let response = client
        .smart_request(
            AiCapability::Analyze,
            "fn fib(n: i32) -> i32 { if n <= 1 { n } else { fib(n-1) + fib(n-2) } }",
        )
        .await?;
    println!("ğŸ§  åˆ†æç»“æœ:\n{}", response.content);

    // æµ‹è¯•2: æ™ºèƒ½Gitæäº¤
    println!("\nğŸ“ æµ‹è¯•2: Gitæ™ºèƒ½æäº¤");
    let diff_context = "ä¿®æ”¹äº†é”™è¯¯å¤„ç†é€»è¾‘ï¼Œæ·»åŠ äº†ç”¨æˆ·è¾“å…¥éªŒè¯";
    let commit = client.smart_commit(diff_context).await?;
    println!("ğŸ¯ ç”Ÿæˆçš„æäº¤ä¿¡æ¯: {}", commit.content);

    // æµ‹è¯•3: ä»£ç å®¡æŸ¥
    println!("\nğŸ” æµ‹è¯•3: ä»£ç å®¡æŸ¥");
    let review = client
        .code_review("fn divide(a: i32, b: i32) -> i32 { a / b }", "math.rs")
        .await?;
    println!("ğŸ“‹ å®¡æŸ¥ç»“æœ:\n{}", review.content);

    println!("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GXL AI-Native éªŒè¯æˆåŠŸ");
    Ok(())
}
