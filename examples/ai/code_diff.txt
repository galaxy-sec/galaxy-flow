"生成 git commit message "
diff --git a/src/ability/ai.rs b/src/ability/ai.rs
index 017b3b9..625a6ac 100644
--- a/src/ability/ai.rs
+++ b/src/ability/ai.rs
@@ -37,17 +37,15 @@ impl GxAIChat {
 
         let message = if let Some(prompt_file) = &self.prompt_file {
             let prompt_file = PathBuf::from(exp.eval(prompt_file)?);
-            if prompt_file.exists() {
-                return ExecReason::from_logic(format!(
-                    "unsupport this format {}",
-                    prompt_file.display()
-                ))
-                .err_result();
+            if !prompt_file.exists() {
+                return ExecReason::from_logic(format!("{} not exists", prompt_file.display()))
+                    .err_result();
             }
             //std::fs::read_to_string(prompt_file.as_path()).owe(AiErrReason::from(
             //    UvsReason::DataError("prompat file read error".into(), None),
             //))?
-            std::fs::read_to_string(prompt_file.as_path()).map_err(|e| ExecReason::from_res(format!("prompt_file:{e}")))?
+            std::fs::read_to_string(prompt_file.as_path())
+                .map_err(|e| ExecReason::from_res(format!("prompt_file:{e}")))?
         } else if let Some(prompt_msg) = &self.prompt_msg {
             prompt_msg.clone()
         } else {
@@ -85,6 +83,19 @@ impl GxAIChat {
 #[cfg(test)]
 mod tests {
 
+    use orion_error::TestAssert;
+
     use crate::{
+        ability::{ability_env_init, ai::GxAIChat, prelude::AsyncRunnableTrait},
+        traits::Setter,
+        util::OptionFrom,
     };
+    #[tokio::test]
+    async fn ai_chat() {
+        let (context, mut def) = ability_env_init();
+        def.global_mut()
+            .set("CONF_ROOT", "${GXL_PRJ_ROOT}/tests/material");
+        let res = GxAIChat::default().with_prompt_msg(" 1 + 1 = ?".to_opt());
+        let _ = res.async_exec(context, def).await.assert();
+    }
 }
diff --git a/src/ai/config/structures.rs b/src/ai/config/structures.rs
index 0db7cd2..eda84c4 100644
--- a/src/ai/config/structures.rs
+++ b/src/ai/config/structures.rs
@@ -1,5 +1,5 @@
 use orion_common::serde::Yamlable;
-use orion_error::{UvsConfFrom, UvsResFrom};
+use orion_error::{ToStructError, UvsConfFrom, UvsResFrom};
 use orion_variate::vars::{EnvDict, EnvEvalable};
 use serde::{Deserialize, Serialize};
 use std::collections::HashMap;
@@ -50,11 +50,16 @@ impl AiConfig {
         let galaxy_dir = home_dir()
             .ok_or_else(|| ExecReason::from_res("Cannot find home directory".into()))?
             .join(".galaxy");
-        let ai_conf_path = galaxy_dir.join(AI_CONF_FILE);
-        if !ai_conf_path.exists() {
-            todo!();
-        }
-        let conf = AiConfig::from_yml(&ai_conf_path)
+        let gal_ai_conf = galaxy_dir.join(AI_CONF_FILE);
+        let prj_ai_conf = PathBuf::from("./_gal").join(AI_CONF_FILE);
+        let ai_conf = if prj_ai_conf.exists() {
+            prj_ai_conf
+        } else if gal_ai_conf.exists() {
+            gal_ai_conf
+        } else {
+            return ExecReason::from_conf("miss ai config".to_string()).err_result();
+        };
+        let conf = AiConfig::from_yml(&ai_conf)
             .map_err(|e| ExecReason::from_conf(format!("ai_conf :{e}")))?;
         Ok(conf.env_eval(dict))
     }
@@ -69,7 +74,7 @@ impl AiConfig {
             AiProviderType::OpenAi,
             ProviderConfig {
                 enabled: true,
-                api_key_env: "${OPENAI_API_KEY}".to_string(),
+                api_key: "${OPENAI_API_KEY}".to_string(),
                 base_url: Some("https://api.openai.com/v1".to_string()),
                 timeout: 30,
                 model_aliases: None,
@@ -82,7 +87,7 @@ impl AiConfig {
             AiProviderType::DeepSeek,
             ProviderConfig {
                 enabled: true,
-                api_key_env: "${DEEPSEEK_API_KEY}".to_string(),
+                api_key: "${DEEPSEEK_API_KEY}".to_string(),
                 base_url: Some("https://api.deepseek.com/v1".to_string()),
                 timeout: 30,
                 model_aliases: None,
@@ -95,7 +100,7 @@ impl AiConfig {
             AiProviderType::Glm,
             ProviderConfig {
                 enabled: true,
-                api_key_env: "${GLM_API_KEY}".to_string(),
+                api_key: "${GLM_API_KEY}".to_string(),
                 base_url: Some("https://open.bigmodel.cn/api/paas/v4".to_string()),
                 timeout: 30,
                 model_aliases: None,
@@ -108,7 +113,7 @@ impl AiConfig {
             AiProviderType::Kimi,
             ProviderConfig {
                 enabled: true,
-                api_key_env: "${KIMI_API_KEY}".to_string(),
+                api_key: "${KIMI_API_KEY}".to_string(),
                 base_url: Some("https://api.moonshot.cn/v1".to_string()),
                 timeout: 30,
                 model_aliases: None,
@@ -133,7 +138,7 @@ impl AiConfig {
             AiProviderType::OpenAi,
             ProviderConfig {
                 enabled: true,
-                api_key_env: "OPENAI_API_KEY".to_string(),
+                api_key: "${OPENAI_API_KEY}".to_string(),
                 base_url: Some("https://api.openai.com/v1".to_string()),
                 timeout: 30,
                 model_aliases: None,
@@ -145,7 +150,7 @@ impl AiConfig {
             AiProviderType::DeepSeek,
             ProviderConfig {
                 enabled: true,
-                api_key_env: "DEEPSEEK_API_KEY".to_string(),
+                api_key: "${DEEPSEEK_API_KEY}".to_string(),
                 base_url: Some("https://api.deepseek.com/v1".to_string()),
                 timeout: 30,
                 model_aliases: None,
@@ -157,7 +162,7 @@ impl AiConfig {
             AiProviderType::Groq,
             ProviderConfig {
                 enabled: false,
-                api_key_env: "GROQ_API_KEY".to_string(),
+                api_key: "${GROQ_API_KEY}".to_string(),
                 base_url: Some("https://api.groq.com/openai/v1".to_string()),
                 timeout: 30,
                 model_aliases: None,
@@ -169,7 +174,7 @@ impl AiConfig {
             AiProviderType::Mock,
             ProviderConfig {
                 enabled: true,
-                api_key_env: "MOCK_API_KEY".to_string(),
+                api_key: "mock".to_string(),
                 base_url: None,
                 timeout: 30,
                 model_aliases: None,
@@ -181,7 +186,7 @@ impl AiConfig {
             AiProviderType::Anthropic,
             ProviderConfig {
                 enabled: false,
-                api_key_env: "CLAUDE_API_KEY".to_string(),
+                api_key: "${CLAUDE_API_KEY}".to_string(),
                 base_url: None,
                 timeout: 30,
                 model_aliases: None,
@@ -193,7 +198,7 @@ impl AiConfig {
             AiProviderType::Ollama,
             ProviderConfig {
                 enabled: false,
-                api_key_env: "OLLAMA_MODEL".to_string(),
+                api_key: "${OLLAMA_MODEL}".to_string(),
                 base_url: Some("http://localhost:11434".to_string()),
                 timeout: 30,
                 model_aliases: None,
@@ -213,7 +218,8 @@ impl AiConfig {
     pub fn get_api_key(&self, provider: AiProviderType) -> Option<String> {
         if let Some(config) = self.providers.get(&provider) {
             if config.enabled {
-                std::env::var(&config.api_key_env).ok()
+                // 直接返回 api_key 值，变量替换已经在 env_eval 中实现
+                Some(config.api_key.clone())
             } else {
                 None
             }
@@ -257,7 +263,7 @@ impl EnvEvalable<FileConfig> for FileConfig {
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct ProviderConfig {
     pub enabled: bool,
-    pub api_key_env: String,
+    pub api_key: String,
     pub base_url: Option<String>,
     pub timeout: u64,
     pub model_aliases: Option<HashMap<String, String>>,
@@ -266,13 +272,13 @@ pub struct ProviderConfig {
 
 impl EnvEvalable<ProviderConfig> for ProviderConfig {
     fn env_eval(self, dict: &EnvDict) -> Self {
-        let api_key_env = self.api_key_env.env_eval(dict);
+        let api_key = self.api_key.env_eval(dict);
         let base_url = Self::eval_base_url(self.base_url, dict);
         let model_aliases = Self::eval_model_aliases(self.model_aliases, dict);
 
         Self {
             enabled: self.enabled,
-            api_key_env,
+            api_key,
             base_url,
             timeout: self.timeout,
             model_aliases,
@@ -338,7 +344,7 @@ impl Default for ProviderConfig {
     fn default() -> Self {
         Self {
             enabled: true,
-            api_key_env: "OPENAI_API_KEY".to_string(),
+            api_key: "${OPENAI_API_KEY}".to_string(),
             base_url: None,
             timeout: 30,
             model_aliases: None,
diff --git a/src/ai/config/tests.rs b/src/ai/config/tests.rs
index e1b65ae..a8d015e 100644
--- a/src/ai/config/tests.rs
+++ b/src/ai/config/tests.rs
@@ -50,31 +50,17 @@ default: ${NON_EXISTENT:-default_value}"#;
     assert!(evaluated.contains("default_value"));
 }
 
-#[test]
-fn test_config_file_not_found() {
-    let loader = ConfigLoader::new();
-
-    // 删除可能存在的配置文件
-    let config_path = ConfigLoader::ensure_config_dir().unwrap().join("ai.yml");
-    if config_path.exists() {
-        std::fs::remove_file(&config_path).unwrap();
-    }
-
-    let result = loader.load_file_config();
-    assert!(result.is_err());
-}
-
 #[test]
 fn test_get_api_key() {
     std::env::set_var("OPENAI_API_KEY", "test_openai_key");
     std::env::set_var("MOCK_API_KEY", "mock_value");
 
-    let config = AiConfig::default();
+    let config = AiConfig::from_env();
 
     // 测试获取存在的API密钥
     assert_eq!(
         config.get_api_key(AiProviderType::OpenAi),
-        Some("test_openai_key".to_string())
+        Some("${OPENAI_API_KEY}".to_string())
     );
 
     // 测试获取不存在的API密钥
@@ -83,7 +69,7 @@ fn test_get_api_key() {
     // 测试Mock provider
     assert_eq!(
         config.get_api_key(AiProviderType::Mock),
-        Some("mock_value".to_string())
+        Some("mock".to_string())
     );
 }
 
@@ -160,7 +146,7 @@ fn test_env_evalable_recursive() {
 
     // 为 ProviderConfig 设置带有变量的值
     let openai_config = config.providers.get_mut(&AiProviderType::OpenAi).unwrap();
-    openai_config.api_key_env = "${OPENAI_API_KEY:-default_key}".to_string();
+    openai_config.api_key = "${OPENAI_API_KEY:-default_key}".to_string();
     openai_config.base_url = Some("${BASE_URL:-https://api.openai.com/v1}".to_string());
 
     // 为 routing 设置带有变量的值
@@ -181,7 +167,7 @@ fn test_env_evalable_recursive() {
         .providers
         .get(&AiProviderType::OpenAi)
         .unwrap();
-    assert_eq!(openai_config.api_key_env, "real_api_key");
+    assert_eq!(openai_config.api_key, "real_api_key");
     assert_eq!(
         openai_config.base_url,
         Some("https://custom.api.com/v1".to_string())
@@ -281,19 +267,19 @@ fn test_config_example() {
     // 检查配置是否启用
     let openai_config = config.providers.get(&AiProviderType::OpenAi).unwrap();
     assert!(openai_config.enabled);
-    assert_eq!(openai_config.api_key_env, "${OPENAI_API_KEY}");
+    assert_eq!(openai_config.api_key, "${OPENAI_API_KEY}");
 
     let deepseek_config = config.providers.get(&AiProviderType::DeepSeek).unwrap();
     assert!(deepseek_config.enabled);
-    assert_eq!(deepseek_config.api_key_env, "${DEEPSEEK_API_KEY}");
+    assert_eq!(deepseek_config.api_key, "${DEEPSEEK_API_KEY}");
 
     let glm_config = config.providers.get(&AiProviderType::Glm).unwrap();
     assert!(glm_config.enabled);
-    assert_eq!(glm_config.api_key_env, "${GLM_API_KEY}");
+    assert_eq!(glm_config.api_key, "${GLM_API_KEY}");
 
     let kimi_config = config.providers.get(&AiProviderType::Kimi).unwrap();
     assert!(kimi_config.enabled);
-    assert_eq!(kimi_config.api_key_env, "${KIMI_API_KEY}");
+    assert_eq!(kimi_config.api_key, "${KIMI_API_KEY}");
 
     // 检查路由和限制配置
     assert_eq!(config.routing.simple, "gpt-4o-mini");
diff --git a/src/ai/providers/client_deepseek_tests.rs b/src/ai/providers/client_deepseek_tests.rs
index 2fb04b0..ea67e87 100644
--- a/src/ai/providers/client_deepseek_tests.rs
+++ b/src/ai/providers/client_deepseek_tests.rs
@@ -45,7 +45,7 @@ mod basic {
         let config = AiConfig::from_env(); // 使用 from_env 而不是 example，因为 example 不会读取环境变量
         assert_eq!(
             config.get_api_key(AiProviderType::DeepSeek),
-            Some("valid_test_key".to_string())
+            Some("${DEEPSEEK_API_KEY}".to_string())
         );
 
         // 测试空的环境变量
diff --git a/src/model/components/gxl_block.rs b/src/model/components/gxl_block.rs
index ec03aa5..34f994f 100644
--- a/src/model/components/gxl_block.rs
+++ b/src/model/components/gxl_block.rs
@@ -7,6 +7,7 @@ use async_trait::async_trait;
 use derive_more::From;
 use std::sync::mpsc::Sender;
 
+use crate::ability::ai::GxAIChat;
 use crate::ability::archive::GxTar;
 use crate::ability::archive::GxUnTar;
 use crate::ability::delegate::ActCall;
@@ -23,6 +24,7 @@ use crate::util::redirect::ReadSignal;
 
 #[derive(Clone, From)]
 pub enum BlockAction {
+    AiChat(GxAIChat),
     Shell(GxShell),
     Command(GxCmd),
     GxlRun(GxRun),
@@ -70,6 +72,7 @@ impl AsyncRunnableWithSenderTrait for BlockAction {
         sender: Option<Sender<ReadSignal>>,
     ) -> TaskResult {
         match self {
+            BlockAction::AiChat(o) => o.async_exec(ctx, dct).await,
             BlockAction::GxlRun(o) => o.async_exec(ctx, dct, sender).await,
             BlockAction::Loop(o) => o.async_exec(ctx, dct, sender).await,
             BlockAction::Shell(o) => o.async_exec(ctx, dct).await,
@@ -124,6 +127,7 @@ impl DependTrait<&GxlSpace> for BlockNode {
         };
         for x in self.items {
             let item = match x {
+                BlockAction::AiChat(v) => BlockAction::AiChat(v.clone()),
                 BlockAction::Tpl(v) => BlockAction::Tpl(v.clone()),
                 BlockAction::Tar(v) => BlockAction::Tar(v.clone()),
                 BlockAction::UnTar(v) => BlockAction::UnTar(v.clone()),
@@ -131,7 +135,6 @@ impl DependTrait<&GxlSpace> for BlockNode {
                 BlockAction::Loop(v) => BlockAction::Loop(v.clone()),
                 BlockAction::Read(v) => BlockAction::Read(v.clone()),
                 BlockAction::Echo(v) => BlockAction::Echo(v.clone()),
-                //BlockAction::Vault(v) => BlockAction::Vault(v.clone()),
                 BlockAction::Assert(v) => BlockAction::Assert(v.clone()),
                 BlockAction::Version(v) => BlockAction::Version(v.clone()),
                 BlockAction::Command(v) => BlockAction::Command(v.clone()),
diff --git a/src/parser/inner/mod.rs b/src/parser/inner/mod.rs
index 79ec736..6ca9530 100644
--- a/src/parser/inner/mod.rs
+++ b/src/parser/inner/mod.rs
@@ -13,6 +13,7 @@ pub mod ver;
 pub use assert::gal_assert;
 pub use cmd::gal_cmd;
 
+pub mod ai;
 pub use common::*;
 pub use load::*;
 pub use read::*;
diff --git a/src/parser/stc_blk.rs b/src/parser/stc_blk.rs
index 9f4b7c2..cde1705 100644
--- a/src/parser/stc_blk.rs
+++ b/src/parser/stc_blk.rs
@@ -1,3 +1,4 @@
+use super::inner::ai::gal_ai_chat;
 use super::inner::call::gal_call;
 use super::inner::cmd::gal_cmd_block;
 use super::inner::gxl::gal_run;
@@ -68,6 +69,9 @@ pub fn gal_sentens_item(input: &mut &str) -> Result<BlockAction> {
     if starts_with("gx.shell", input) {
         return gal_shell.map(BlockAction::Shell).parse_next(input);
     }
+    if starts_with("gx.ai_chat", input) {
+        return gal_ai_chat.map(BlockAction::AiChat).parse_next(input);
+    }
 
     if starts_with("gx.run", input) {
         return gal_run.map(BlockAction::GxlRun).parse_next(input);
