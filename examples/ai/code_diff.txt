diff --git a/examples/ai/code_diff.txt b/examples/ai/code_diff.txt
index 343b9c1..e69de29 100644
--- a/examples/ai/code_diff.txt
+++ b/examples/ai/code_diff.txt
@@ -1,458 +0,0 @@
-diff --git a/AI_COMPLETE.md b/AI_COMPLETE.md
-index 806341f..9535cfd 100644
---- a/AI_COMPLETE.md
-+++ b/AI_COMPLETE.md
-@@ -355,3 +355,5 @@ cargo run -- --model mixtral "快速推理"
- 4. 通过配置文件精细控制AI Provider的行为
- 
- **主要价值**: 通过配置文件实现了Provider选择的环境变量替换支持，为用户提供了更灵活、更可维护的配置管理方式。
-+
-+[] 在 _gal 目录下建立 AI 规则文件，在AiClient 时可以加载使用
-diff --git a/examples/ai/_gal/work.gxl b/examples/ai/_gal/work.gxl
-index dff570c..c5dd255 100644
---- a/examples/ai/_gal/work.gxl
-+++ b/examples/ai/_gal/work.gxl
-@@ -8,9 +8,8 @@ mod main   {
-   }
- 
-   flow ci_msg {
--    gx.cmd("echo \"生成 git commit message \" > code_diff.txt");
--    gx.cmd("git diff >> code_diff.txt");
--    gx.ai_chat( prompt_file : "./code_diff.txt");
-+    gx.cmd("git diff > code_diff.txt");
-+    gx.ai_chat( "生成简要的 git commit message", prompt_file : "./code_diff.txt");
-   }
- 
- }
-diff --git a/examples/ai/code_diff.txt b/examples/ai/code_diff.txt
-index 2a5a762..e69de29 100644
---- a/examples/ai/code_diff.txt
-+++ b/examples/ai/code_diff.txt
-@@ -1,392 +0,0 @@
--"生成 git commit message "
--diff --git a/src/ability/ai.rs b/src/ability/ai.rs
--index 017b3b9..625a6ac 100644
----- a/src/ability/ai.rs
--+++ b/src/ability/ai.rs
--@@ -37,17 +37,15 @@ impl GxAIChat {
-- 
--         let message = if let Some(prompt_file) = &self.prompt_file {
--             let prompt_file = PathBuf::from(exp.eval(prompt_file)?);
---            if prompt_file.exists() {
---                return ExecReason::from_logic(format!(
---                    "unsupport this format {}",
---                    prompt_file.display()
---                ))
---                .err_result();
--+            if !prompt_file.exists() {
--+                return ExecReason::from_logic(format!("{} not exists", prompt_file.display()))
--+                    .err_result();
--             }
--             //std::fs::read_to_string(prompt_file.as_path()).owe(AiErrReason::from(
--             //    UvsReason::DataError("prompat file read error".into(), None),
--             //))?
---            std::fs::read_to_string(prompt_file.as_path()).map_err(|e| ExecReason::from_res(format!("prompt_file:{e}")))?
--+            std::fs::read_to_string(prompt_file.as_path())
--+                .map_err(|e| ExecReason::from_res(format!("prompt_file:{e}")))?
--         } else if let Some(prompt_msg) = &self.prompt_msg {
--             prompt_msg.clone()
--         } else {
--@@ -85,6 +83,19 @@ impl GxAIChat {
-- #[cfg(test)]
-- mod tests {
-- 
--+    use orion_error::TestAssert;
--+
--     use crate::{
--+        ability::{ability_env_init, ai::GxAIChat, prelude::AsyncRunnableTrait},
--+        traits::Setter,
--+        util::OptionFrom,
--     };
--+    #[tokio::test]
--+    async fn ai_chat() {
--+        let (context, mut def) = ability_env_init();
--+        def.global_mut()
--+            .set("CONF_ROOT", "${GXL_PRJ_ROOT}/tests/material");
--+        let res = GxAIChat::default().with_prompt_msg(" 1 + 1 = ?".to_opt());
--+        let _ = res.async_exec(context, def).await.assert();
--+    }
-- }
--diff --git a/src/ai/config/structures.rs b/src/ai/config/structures.rs
--index 0db7cd2..eda84c4 100644
----- a/src/ai/config/structures.rs
--+++ b/src/ai/config/structures.rs
--@@ -1,5 +1,5 @@
-- use orion_common::serde::Yamlable;
---use orion_error::{UvsConfFrom, UvsResFrom};
--+use orion_error::{ToStructError, UvsConfFrom, UvsResFrom};
-- use orion_variate::vars::{EnvDict, EnvEvalable};
-- use serde::{Deserialize, Serialize};
-- use std::collections::HashMap;
--@@ -50,11 +50,16 @@ impl AiConfig {
--         let galaxy_dir = home_dir()
--             .ok_or_else(|| ExecReason::from_res("Cannot find home directory".into()))?
--             .join(".galaxy");
---        let ai_conf_path = galaxy_dir.join(AI_CONF_FILE);
---        if !ai_conf_path.exists() {
---            todo!();
---        }
---        let conf = AiConfig::from_yml(&ai_conf_path)
--+        let gal_ai_conf = galaxy_dir.join(AI_CONF_FILE);
--+        let prj_ai_conf = PathBuf::from("./_gal").join(AI_CONF_FILE);
--+        let ai_conf = if prj_ai_conf.exists() {
--+            prj_ai_conf
--+        } else if gal_ai_conf.exists() {
--+            gal_ai_conf
--+        } else {
--+            return ExecReason::from_conf("miss ai config".to_string()).err_result();
--+        };
--+        let conf = AiConfig::from_yml(&ai_conf)
--             .map_err(|e| ExecReason::from_conf(format!("ai_conf :{e}")))?;
--         Ok(conf.env_eval(dict))
--     }
--@@ -69,7 +74,7 @@ impl AiConfig {
--             AiProviderType::OpenAi,
--             ProviderConfig {
--                 enabled: true,
---                api_key_env: "${OPENAI_API_KEY}".to_string(),
--+                api_key: "${OPENAI_API_KEY}".to_string(),
--                 base_url: Some("https://api.openai.com/v1".to_string()),
--                 timeout: 30,
--                 model_aliases: None,
--@@ -82,7 +87,7 @@ impl AiConfig {
--             AiProviderType::DeepSeek,
--             ProviderConfig {
--                 enabled: true,
---                api_key_env: "${DEEPSEEK_API_KEY}".to_string(),
--+                api_key: "${DEEPSEEK_API_KEY}".to_string(),
--                 base_url: Some("https://api.deepseek.com/v1".to_string()),
--                 timeout: 30,
--                 model_aliases: None,
--@@ -95,7 +100,7 @@ impl AiConfig {
--             AiProviderType::Glm,
--             ProviderConfig {
--                 enabled: true,
---                api_key_env: "${GLM_API_KEY}".to_string(),
--+                api_key: "${GLM_API_KEY}".to_string(),
--                 base_url: Some("https://open.bigmodel.cn/api/paas/v4".to_string()),
--                 timeout: 30,
--                 model_aliases: None,
--@@ -108,7 +113,7 @@ impl AiConfig {
--             AiProviderType::Kimi,
--             ProviderConfig {
--                 enabled: true,
---                api_key_env: "${KIMI_API_KEY}".to_string(),
--+                api_key: "${KIMI_API_KEY}".to_string(),
--                 base_url: Some("https://api.moonshot.cn/v1".to_string()),
--                 timeout: 30,
--                 model_aliases: None,
--@@ -133,7 +138,7 @@ impl AiConfig {
--             AiProviderType::OpenAi,
--             ProviderConfig {
--                 enabled: true,
---                api_key_env: "OPENAI_API_KEY".to_string(),
--+                api_key: "${OPENAI_API_KEY}".to_string(),
--                 base_url: Some("https://api.openai.com/v1".to_string()),
--                 timeout: 30,
--                 model_aliases: None,
--@@ -145,7 +150,7 @@ impl AiConfig {
--             AiProviderType::DeepSeek,
--             ProviderConfig {
--                 enabled: true,
---                api_key_env: "DEEPSEEK_API_KEY".to_string(),
--+                api_key: "${DEEPSEEK_API_KEY}".to_string(),
--                 base_url: Some("https://api.deepseek.com/v1".to_string()),
--                 timeout: 30,
--                 model_aliases: None,
--@@ -157,7 +162,7 @@ impl AiConfig {
--             AiProviderType::Groq,
--             ProviderConfig {
--                 enabled: false,
---                api_key_env: "GROQ_API_KEY".to_string(),
--+                api_key: "${GROQ_API_KEY}".to_string(),
--                 base_url: Some("https://api.groq.com/openai/v1".to_string()),
--                 timeout: 30,
--                 model_aliases: None,
--@@ -169,7 +174,7 @@ impl AiConfig {
--             AiProviderType::Mock,
--             ProviderConfig {
--                 enabled: true,
---                api_key_env: "MOCK_API_KEY".to_string(),
--+                api_key: "mock".to_string(),
--                 base_url: None,
--                 timeout: 30,
--                 model_aliases: None,
--@@ -181,7 +186,7 @@ impl AiConfig {
--             AiProviderType::Anthropic,
--             ProviderConfig {
--                 enabled: false,
---                api_key_env: "CLAUDE_API_KEY".to_string(),
--+                api_key: "${CLAUDE_API_KEY}".to_string(),
--                 base_url: None,
--                 timeout: 30,
--                 model_aliases: None,
--@@ -193,7 +198,7 @@ impl AiConfig {
--             AiProviderType::Ollama,
--             ProviderConfig {
--                 enabled: false,
---                api_key_env: "OLLAMA_MODEL".to_string(),
--+                api_key: "${OLLAMA_MODEL}".to_string(),
--                 base_url: Some("http://localhost:11434".to_string()),
--                 timeout: 30,
--                 model_aliases: None,
--@@ -213,7 +218,8 @@ impl AiConfig {
--     pub fn get_api_key(&self, provider: AiProviderType) -> Option<String> {
--         if let Some(config) = self.providers.get(&provider) {
--             if config.enabled {
---                std::env::var(&config.api_key_env).ok()
--+                // 直接返回 api_key 值，变量替换已经在 env_eval 中实现
--+                Some(config.api_key.clone())
--             } else {
--                 None
--             }
--@@ -257,7 +263,7 @@ impl EnvEvalable<FileConfig> for FileConfig {
-- #[derive(Debug, Clone, Serialize, Deserialize)]
-- pub struct ProviderConfig {
--     pub enabled: bool,
---    pub api_key_env: String,
--+    pub api_key: String,
--     pub base_url: Option<String>,
--     pub timeout: u64,
--     pub model_aliases: Option<HashMap<String, String>>,
--@@ -266,13 +272,13 @@ pub struct ProviderConfig {
-- 
-- impl EnvEvalable<ProviderConfig> for ProviderConfig {
--     fn env_eval(self, dict: &EnvDict) -> Self {
---        let api_key_env = self.api_key_env.env_eval(dict);
--+        let api_key = self.api_key.env_eval(dict);
--         let base_url = Self::eval_base_url(self.base_url, dict);
--         let model_aliases = Self::eval_model_aliases(self.model_aliases, dict);
-- 
--         Self {
--             enabled: self.enabled,
---            api_key_env,
--+            api_key,
--             base_url,
--             timeout: self.timeout,
--             model_aliases,
--@@ -338,7 +344,7 @@ impl Default for ProviderConfig {
--     fn default() -> Self {
--         Self {
--             enabled: true,
---            api_key_env: "OPENAI_API_KEY".to_string(),
--+            api_key: "${OPENAI_API_KEY}".to_string(),
--             base_url: None,
--             timeout: 30,
--             model_aliases: None,
--diff --git a/src/ai/config/tests.rs b/src/ai/config/tests.rs
--index e1b65ae..a8d015e 100644
----- a/src/ai/config/tests.rs
--+++ b/src/ai/config/tests.rs
--@@ -50,31 +50,17 @@ default: ${NON_EXISTENT:-default_value}"#;
--     assert!(evaluated.contains("default_value"));
-- }
-- 
---#[test]
---fn test_config_file_not_found() {
---    let loader = ConfigLoader::new();
---
---    // 删除可能存在的配置文件
---    let config_path = ConfigLoader::ensure_config_dir().unwrap().join("ai.yml");
---    if config_path.exists() {
---        std::fs::remove_file(&config_path).unwrap();
---    }
---
---    let result = loader.load_file_config();
---    assert!(result.is_err());
---}
---
-- #[test]
-- fn test_get_api_key() {
--     std::env::set_var("OPENAI_API_KEY", "test_openai_key");
--     std::env::set_var("MOCK_API_KEY", "mock_value");
-- 
---    let config = AiConfig::default();
--+    let config = AiConfig::from_env();
-- 
--     // 测试获取存在的API密钥
--     assert_eq!(
--         config.get_api_key(AiProviderType::OpenAi),
---        Some("test_openai_key".to_string())
--+        Some("${OPENAI_API_KEY}".to_string())
--     );
-- 
--     // 测试获取不存在的API密钥
--@@ -83,7 +69,7 @@ fn test_get_api_key() {
--     // 测试Mock provider
--     assert_eq!(
--         config.get_api_key(AiProviderType::Mock),
---        Some("mock_value".to_string())
--+        Some("mock".to_string())
--     );
-- }
-- 
--@@ -160,7 +146,7 @@ fn test_env_evalable_recursive() {
-- 
--     // 为 ProviderConfig 设置带有变量的值
--     let openai_config = config.providers.get_mut(&AiProviderType::OpenAi).unwrap();
---    openai_config.api_key_env = "${OPENAI_API_KEY:-default_key}".to_string();
--+    openai_config.api_key = "${OPENAI_API_KEY:-default_key}".to_string();
--     openai_config.base_url = Some("${BASE_URL:-https://api.openai.com/v1}".to_string());
-- 
--     // 为 routing 设置带有变量的值
--@@ -181,7 +167,7 @@ fn test_env_evalable_recursive() {
--         .providers
--         .get(&AiProviderType::OpenAi)
--         .unwrap();
---    assert_eq!(openai_config.api_key_env, "real_api_key");
--+    assert_eq!(openai_config.api_key, "real_api_key");
--     assert_eq!(
--         openai_config.base_url,
--         Some("https://custom.api.com/v1".to_string())
--@@ -281,19 +267,19 @@ fn test_config_example() {
--     // 检查配置是否启用
--     let openai_config = config.providers.get(&AiProviderType::OpenAi).unwrap();
--     assert!(openai_config.enabled);
---    assert_eq!(openai_config.api_key_env, "${OPENAI_API_KEY}");
--+    assert_eq!(openai_config.api_key, "${OPENAI_API_KEY}");
-- 
--     let deepseek_config = config.providers.get(&AiProviderType::DeepSeek).unwrap();
--     assert!(deepseek_config.enabled);
---    assert_eq!(deepseek_config.api_key_env, "${DEEPSEEK_API_KEY}");
--+    assert_eq!(deepseek_config.api_key, "${DEEPSEEK_API_KEY}");
-- 
--     let glm_config = config.providers.get(&AiProviderType::Glm).unwrap();
--     assert!(glm_config.enabled);
---    assert_eq!(glm_config.api_key_env, "${GLM_API_KEY}");
--+    assert_eq!(glm_config.api_key, "${GLM_API_KEY}");
-- 
--     let kimi_config = config.providers.get(&AiProviderType::Kimi).unwrap();
--     assert!(kimi_config.enabled);
---    assert_eq!(kimi_config.api_key_env, "${KIMI_API_KEY}");
--+    assert_eq!(kimi_config.api_key, "${KIMI_API_KEY}");
-- 
--     // 检查路由和限制配置
--     assert_eq!(config.routing.simple, "gpt-4o-mini");
--diff --git a/src/ai/providers/client_deepseek_tests.rs b/src/ai/providers/client_deepseek_tests.rs
--index 2fb04b0..ea67e87 100644
----- a/src/ai/providers/client_deepseek_tests.rs
--+++ b/src/ai/providers/client_deepseek_tests.rs
--@@ -45,7 +45,7 @@ mod basic {
--         let config = AiConfig::from_env(); // 使用 from_env 而不是 example，因为 example 不会读取环境变量
--         assert_eq!(
--             config.get_api_key(AiProviderType::DeepSeek),
---            Some("valid_test_key".to_string())
--+            Some("${DEEPSEEK_API_KEY}".to_string())
--         );
-- 
--         // 测试空的环境变量
--diff --git a/src/model/components/gxl_block.rs b/src/model/components/gxl_block.rs
--index ec03aa5..34f994f 100644
----- a/src/model/components/gxl_block.rs
--+++ b/src/model/components/gxl_block.rs
--@@ -7,6 +7,7 @@ use async_trait::async_trait;
-- use derive_more::From;
-- use std::sync::mpsc::Sender;
-- 
--+use crate::ability::ai::GxAIChat;
-- use crate::ability::archive::GxTar;
-- use crate::ability::archive::GxUnTar;
-- use crate::ability::delegate::ActCall;
--@@ -23,6 +24,7 @@ use crate::util::redirect::ReadSignal;
-- 
-- #[derive(Clone, From)]
-- pub enum BlockAction {
--+    AiChat(GxAIChat),
--     Shell(GxShell),
--     Command(GxCmd),
--     GxlRun(GxRun),
--@@ -70,6 +72,7 @@ impl AsyncRunnableWithSenderTrait for BlockAction {
--         sender: Option<Sender<ReadSignal>>,
--     ) -> TaskResult {
--         match self {
--+            BlockAction::AiChat(o) => o.async_exec(ctx, dct).await,
--             BlockAction::GxlRun(o) => o.async_exec(ctx, dct, sender).await,
--             BlockAction::Loop(o) => o.async_exec(ctx, dct, sender).await,
--             BlockAction::Shell(o) => o.async_exec(ctx, dct).await,
--@@ -124,6 +127,7 @@ impl DependTrait<&GxlSpace> for BlockNode {
--         };
--         for x in self.items {
--             let item = match x {
--+                BlockAction::AiChat(v) => BlockAction::AiChat(v.clone()),
--                 BlockAction::Tpl(v) => BlockAction::Tpl(v.clone()),
--                 BlockAction::Tar(v) => BlockAction::Tar(v.clone()),
--                 BlockAction::UnTar(v) => BlockAction::UnTar(v.clone()),
--@@ -131,7 +135,6 @@ impl DependTrait<&GxlSpace> for BlockNode {
--                 BlockAction::Loop(v) => BlockAction::Loop(v.clone()),
--                 BlockAction::Read(v) => BlockAction::Read(v.clone()),
--                 BlockAction::Echo(v) => BlockAction::Echo(v.clone()),
---                //BlockAction::Vault(v) => BlockAction::Vault(v.clone()),
--                 BlockAction::Assert(v) => BlockAction::Assert(v.clone()),
--                 BlockAction::Version(v) => BlockAction::Version(v.clone()),
--                 BlockAction::Command(v) => BlockAction::Command(v.clone()),
--diff --git a/src/parser/inner/mod.rs b/src/parser/inner/mod.rs
--index 79ec736..6ca9530 100644
----- a/src/parser/inner/mod.rs
--+++ b/src/parser/inner/mod.rs
--@@ -13,6 +13,7 @@ pub mod ver;
-- pub use assert::gal_assert;
-- pub use cmd::gal_cmd;
-- 
--+pub mod ai;
-- pub use common::*;
-- pub use load::*;
-- pub use read::*;
--diff --git a/src/parser/stc_blk.rs b/src/parser/stc_blk.rs
--index 9f4b7c2..cde1705 100644
----- a/src/parser/stc_blk.rs
--+++ b/src/parser/stc_blk.rs
--@@ -1,3 +1,4 @@
--+use super::inner::ai::gal_ai_chat;
-- use super::inner::call::gal_call;
-- use super::inner::cmd::gal_cmd_block;
-- use super::inner::gxl::gal_run;
--@@ -68,6 +69,9 @@ pub fn gal_sentens_item(input: &mut &str) -> Result<BlockAction> {
--     if starts_with("gx.shell", input) {
--         return gal_shell.map(BlockAction::Shell).parse_next(input);
--     }
--+    if starts_with("gx.ai_chat", input) {
--+        return gal_ai_chat.map(BlockAction::AiChat).parse_next(input);
--+    }
-- 
--     if starts_with("gx.run", input) {
--         return gal_run.map(BlockAction::GxlRun).parse_next(input);
-diff --git a/src/ability/ai.rs b/src/ability/ai.rs
-index 625a6ac..598658c 100644
---- a/src/ability/ai.rs
-+++ b/src/ability/ai.rs
-@@ -34,23 +34,19 @@ impl GxAIChat {
-         ctx.append("gx.shell");
-         let mut action = Action::from("gx.ai_chat");
-         let exp = EnvExpress::from_env_mix(vars_dict.global().clone());
-+        let mut message = self.prompt_msg.clone().unwrap_or("".to_string());
- 
--        let message = if let Some(prompt_file) = &self.prompt_file {
-+        if let Some(prompt_file) = &self.prompt_file {
-             let prompt_file = PathBuf::from(exp.eval(prompt_file)?);
-             if !prompt_file.exists() {
-                 return ExecReason::from_logic(format!("{} not exists", prompt_file.display()))
-                     .err_result();
-             }
--            //std::fs::read_to_string(prompt_file.as_path()).owe(AiErrReason::from(
--            //    UvsReason::DataError("prompat file read error".into(), None),
--            //))?
--            std::fs::read_to_string(prompt_file.as_path())
--                .map_err(|e| ExecReason::from_res(format!("prompt_file:{e}")))?
--        } else if let Some(prompt_msg) = &self.prompt_msg {
--            prompt_msg.clone()
--        } else {
--            "not need help".to_string()
--        };
-+            let data = std::fs::read_to_string(prompt_file.as_path())
-+                .map_err(|e| ExecReason::from_res(format!("prompt_file:{e}")))?;
-+            message.push_str("\n");
-+            message.push_str(data.as_str());
-+        }
- 
-         // call ai clien
-         let ai_config = AiConfig::galaxy_load(&vars_dict.global().export().into())?;
diff --git a/src/ai/config/roles/loader.rs b/src/ai/config/roles/loader.rs
index 351c11c..33f5bdc 100644
--- a/src/ai/config/roles/loader.rs
+++ b/src/ai/config/roles/loader.rs
@@ -57,10 +57,7 @@ impl RoleConfigLoader {
                     "用户级配置路径转换失败".to_string(),
                 ))
             })?;
-            println!(
-                "📄 Loading user-level roles configuration from {}...",
-                user_roles_str
-            );
+            println!("📄 Loading user-level roles configuration from {user_roles_str}...");
             let mut manager = RoleConfigManager::new(user_roles_str.to_string());
             manager.load_config()?;
             return Ok(manager);
diff --git a/src/ai/config/roles/types.rs b/src/ai/config/roles/types.rs
index 44d5ec7..20aebee 100644
--- a/src/ai/config/roles/types.rs
+++ b/src/ai/config/roles/types.rs
@@ -22,4 +22,4 @@ pub struct RoleConfig {
 pub struct RulesConfig {
     /// 规则集合
     pub rules: Vec<String>,
-}
\ No newline at end of file
+}
diff --git a/src/ai/factory.rs b/src/ai/factory.rs
index 3f52b01..7b9728a 100644
--- a/src/ai/factory.rs
+++ b/src/ai/factory.rs
@@ -21,12 +21,11 @@ impl AiClientEnum {
         let mut validated_config = config.clone();
         validated_config.validate_and_postprocess().map_err(|e| {
             AiError::from(AiErrReason::ConfigError(format!(
-                "Configuration validation failed: {}",
-                e
+                "Configuration validation failed: {e}"
             )))
         })?;
 
-        Ok(AiClient::new(config)?)
+        AiClient::new(config)
     }
 
     /// 创建Thread记录客户端
@@ -47,8 +46,7 @@ impl AiClientEnum {
         let mut validated_config = config;
         validated_config.validate_and_postprocess().map_err(|e| {
             AiError::from(AiErrReason::ConfigError(format!(
-                "Configuration validation failed: {}",
-                e
+                "Configuration validation failed: {e}"
             )))
         })?;
 
diff --git a/src/ai/thread/recorder/client.rs b/src/ai/thread/recorder/client.rs
index 520e2db..15fe926 100644
--- a/src/ai/thread/recorder/client.rs
+++ b/src/ai/thread/recorder/client.rs
@@ -63,7 +63,7 @@ impl ThreadClient {
                     .record_interaction(start_time, &request, resp)
                     .await
                 {
-                    eprintln!("Warning: Failed to record thread interaction: {}", e);
+                    eprintln!("Warning: Failed to record thread interaction: {e}");
                 }
             }
         }
@@ -104,7 +104,7 @@ impl ThreadClient {
                     .record_interaction(start_time, &base_request, resp)
                     .await
                 {
-                    eprintln!("Warning: Failed to record thread interaction: {}", e);
+                    eprintln!("Warning: Failed to record thread interaction: {e}");
                 }
             }
         }
diff --git a/src/ai/thread/recorder/file_manager.rs b/src/ai/thread/recorder/file_manager.rs
index bc848aa..6017ae8 100644
--- a/src/ai/thread/recorder/file_manager.rs
+++ b/src/ai/thread/recorder/file_manager.rs
@@ -58,18 +58,17 @@ impl ThreadFileManager {
     }
 
     /// 解析存储路径中的环境变量
-    fn resolve_storage_path(storage_path: &PathBuf) -> PathBuf {
+    fn resolve_storage_path(storage_path: &Path) -> PathBuf {
         let path_str = storage_path.to_string_lossy();
 
         // 简单的环境变量替换
-        if path_str.starts_with('$') {
-            let env_var = &path_str[1..];
+        if let Some(env_var) = path_str.strip_prefix('$') {
             if let Ok(env_value) = std::env::var(env_var) {
                 return PathBuf::from(env_value);
             }
         }
 
-        storage_path.clone()
+        storage_path.to_path_buf()
     }
 
     /// 生成每日文件路径
@@ -84,7 +83,7 @@ impl ThreadFileManager {
         let filename = if filename.ends_with(".md") {
             filename
         } else {
-            format!("{}.md", filename)
+            format!("{filename}.md")
         };
 
         self.base_path.join(filename)
@@ -158,7 +157,7 @@ impl ThreadFileManager {
         // 如果文件是刚创建的，添加标题
         if !file_exists {
             let date_str = chrono::Utc::now().format("%Y-%m-%d").to_string();
-            let title = format!("# Thread记录 - {}\n\n", date_str);
+            let title = format!("# Thread记录 - {date_str}\n\n");
             file.write_all(title.as_bytes()).await.map_err(|e| {
                 AiError::from(AiErrReason::ContextError(format!(
                     "Failed to write title to file {}: {}",
@@ -240,7 +239,7 @@ mod tests {
 
         // 检查文件是否创建
         let date_str = timestamp.format("%Y-%m-%d").to_string();
-        let expected_file = storage_path.join(format!("test-{}.md", date_str));
+        let expected_file = storage_path.join(format!("test-{date_str}.md"));
         assert!(expected_file.exists());
 
         // 检查文件内容
diff --git a/src/ai/thread/recorder/summary_extractor.rs b/src/ai/thread/recorder/summary_extractor.rs
index 3cb123f..0521e19 100644
--- a/src/ai/thread/recorder/summary_extractor.rs
+++ b/src/ai/thread/recorder/summary_extractor.rs
@@ -40,12 +40,10 @@ impl SummaryExtractor {
 
     /// 寻找包含总结关键字的段落
     fn find_summary_paragraph<'a>(&self, paragraphs: &'a [&str]) -> Option<&'a str> {
-        for &paragraph in paragraphs {
-            if self.contains_summary_keyword(paragraph) {
-                return Some(paragraph);
-            }
-        }
-        None
+        paragraphs
+            .iter()
+            .find(|&&paragraph| self.contains_summary_keyword(paragraph))
+            .copied()
     }
 
     /// 检查文本是否包含总结关键字
diff --git a/tests/thread_integration_test.rs b/tests/thread_integration_test.rs
index ac6b3b2..0e9ef75 100644
--- a/tests/thread_integration_test.rs
+++ b/tests/thread_integration_test.rs
@@ -66,12 +66,12 @@ fn test_thread_integration_basic() {
 
     // 验证Thread文件是否创建
     let date_str = chrono::Utc::now().format("%Y-%m-%d").to_string();
-    let expected_file = storage_path.join(format!("test-thread-{}.md", date_str));
+    let expected_file = storage_path.join(format!("test-thread-{date_str}.md"));
     assert!(expected_file.exists());
 
     // 验证文件内容
     let content = std::fs::read_to_string(&expected_file).unwrap();
-    println!("Thread file content:\n{}", content);
+    println!("Thread file content:\n{content}");
 
     // 检查基本的Thread结构
     assert!(content.contains("# Thread记录"));
@@ -140,12 +140,12 @@ fn test_thread_inform_ai_functionality() {
 
     // 验证Thread文件是否创建
     let date_str = chrono::Utc::now().format("%Y-%m-%d").to_string();
-    let expected_file = storage_path.join(format!("test-inform-{}.md", date_str));
+    let expected_file = storage_path.join(format!("test-inform-{date_str}.md"));
     assert!(expected_file.exists());
 
     // 验证文件内容
     let content = std::fs::read_to_string(&expected_file).unwrap();
-    println!("Thread file content with AI notification:\n{}", content);
+    println!("Thread file content with AI notification:\n{content}");
 
     // 检查基本的Thread结构
     assert!(content.contains("# Thread记录"));
@@ -215,12 +215,12 @@ fn test_thread_without_inform_ai() {
 
     // 验证Thread文件是否创建
     let date_str = chrono::Utc::now().format("%Y-%m-%d").to_string();
-    let expected_file = storage_path.join(format!("test-no-inform-{}.md", date_str));
+    let expected_file = storage_path.join(format!("test-no-inform-{date_str}.md"));
     assert!(expected_file.exists());
 
     // 验证文件内容
     let content = std::fs::read_to_string(&expected_file).unwrap();
-    println!("Thread file content without AI notification:\n{}", content);
+    println!("Thread file content without AI notification:\n{content}");
 
     // 检查基本的Thread结构
     assert!(content.contains("# Thread记录"));
@@ -285,7 +285,7 @@ fn test_thread_integration_with_disabled_config() {
 
     // 验证Thread文件没有被创建
     let date_str = chrono::Utc::now().format("%Y-%m-%d").to_string();
-    let expected_file = storage_path.join(format!("test-thread-{}.md", date_str));
+    let expected_file = storage_path.join(format!("test-thread-{date_str}.md"));
     assert!(!expected_file.exists());
 }
 
diff --git a/version.txt b/version.txt
index d9df1bb..142464b 100644
--- a/version.txt
+++ b/version.txt
@@ -1 +1 @@
-0.11.0
+0.11.0
\ No newline at end of file
