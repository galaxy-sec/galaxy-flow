//! å“åº”è½¬æ¢å™¨æ¨¡å—
//!
//! è¿™ä¸ªæ¨¡å—è´Ÿè´£å°†å„ç§ AI æä¾›å•†çš„å“åº”è½¬æ¢ä¸ºç»Ÿä¸€çš„ AiResponse æ ¼å¼
//! ä¸»è¦åŒ…å«ä» OpenAI æ ¼å¼å“åº”åˆ° AiResponse çš„è½¬æ¢é€»è¾‘

use crate::provider::{AiProviderType, AiResponse, FunctionCall, FunctionCallInfo, UsageInfo};
use crate::providers::openai::OpenAiResponse;
use crate::{AiErrReason, AiResult};
use orion_error::{ErrorOwe, ToStructError, UvsLogicFrom, UvsReason};

/// OpenAI å“åº”è½¬æ¢å™¨
pub struct OpenAiResponseConverter {
    provider_type: AiProviderType,
}

impl OpenAiResponseConverter {
    /// åˆ›å»ºæ–°çš„è½¬æ¢å™¨å®ä¾‹
    pub fn new(provider_type: AiProviderType) -> Self {
        Self { provider_type }
    }

    /// è½¬æ¢ OpenAI å“åº”åˆ° AiResponseï¼ˆä¸å¸¦å‡½æ•°è°ƒç”¨ï¼‰
    ///
    /// è¿™ä¸ªæ–¹æ³•å¯¹åº”äº `send_request` ä¸­çš„å“åº”è½¬æ¢é€»è¾‘
    pub fn convert_response(
        &self,
        openai_response: OpenAiResponse,
        request_model: &str,
        cost_calculator: impl Fn(&str, usize, usize) -> Option<f64>,
    ) -> AiResponse {
        let choice = openai_response
            .choices
            .first()
            .expect("No choices in response");

        AiResponse {
            content: choice.message.content.clone(),
            model: openai_response.model.clone(),
            usage: self.convert_usage(&openai_response, request_model, cost_calculator),
            finish_reason: choice.finish_reason.clone(),
            provider: self.provider_type,
            metadata: std::collections::HashMap::new(),
            tool_calls: None,
        }
    }

    /// è½¬æ¢ OpenAI å“åº”åˆ° AiResponseï¼ˆå¸¦å‡½æ•°è°ƒç”¨ï¼‰
    ///
    /// è¿™ä¸ªæ–¹æ³•å¯¹åº”äº `send_request_with_functions` ä¸­çš„å“åº”è½¬æ¢é€»è¾‘
    pub fn convert_response_with_functions(
        &self,
        openai_response: OpenAiResponse,
        request_model: &str,
        cost_calculator: impl Fn(&str, usize, usize) -> Option<f64>,
    ) -> AiResponse {
        let choice = openai_response
            .choices
            .first()
            .expect("No choices in response");

        let tool_calls = choice.tool_calls.as_ref().map(|tool_calls| {
            tool_calls
                .iter()
                .map(|tool_call| FunctionCall {
                    index: tool_call.index,
                    id: tool_call.id.clone(),
                    r#type: tool_call.r#type.clone(),
                    function: FunctionCallInfo {
                        name: tool_call.function.name.clone(),
                        arguments: tool_call.function.arguments.clone(),
                    },
                })
                .collect()
        });

        AiResponse {
            content: choice.message.content.clone(),
            model: openai_response.model.clone(),
            usage: self.convert_usage(&openai_response, request_model, cost_calculator),
            finish_reason: choice.finish_reason.clone(),
            provider: self.provider_type,
            metadata: std::collections::HashMap::new(),
            tool_calls,
        }
    }

    /// è½¬æ¢ä½¿ç”¨ä¿¡æ¯å’Œæˆæœ¬è®¡ç®—
    fn convert_usage(
        &self,
        openai_response: &OpenAiResponse,
        request_model: &str,
        cost_calculator: impl Fn(&str, usize, usize) -> Option<f64>,
    ) -> UsageInfo {
        let prompt_tokens = openai_response
            .usage
            .as_ref()
            .map(|u| u.prompt_tokens)
            .unwrap_or(0);

        let completion_tokens = openai_response
            .usage
            .as_ref()
            .map(|u| u.completion_tokens)
            .unwrap_or(0);

        let total_tokens = openai_response
            .usage
            .as_ref()
            .map(|u| u.total_tokens)
            .unwrap_or(0);

        let estimated_cost = cost_calculator(request_model, prompt_tokens, completion_tokens);

        UsageInfo {
            prompt_tokens,
            completion_tokens,
            total_tokens,
            estimated_cost,
        }
    }
}

/// é«˜çº§å“åº”è½¬æ¢å‡½æ•°
///
/// ç›´æ¥æ¥æ”¶ JSON å“åº”æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œè‡ªåŠ¨è§£æå’Œè½¬æ¢
/// è¿™æ˜¯æœ€ä¾¿æ·çš„è½¬æ¢å‡½æ•°ï¼Œå°è£…äº† JSON è§£æå’Œå“åº”è½¬æ¢çš„å®Œæ•´æµç¨‹
pub fn convert_response_from_text(
    response_text: &str,
    provider_type: AiProviderType,
    request_model: &str,
    cost_calculator: impl Fn(&str, usize, usize) -> Option<f64>,
) -> AiResult<AiResponse> {
    // é¦–å…ˆè§£æ JSON æ–‡æœ¬
    let openai_response: OpenAiResponse = serde_json::from_str(response_text).owe_data()?;

    // ç„¶åä½¿ç”¨è‡ªåŠ¨è½¬æ¢é€»è¾‘
    convert_response_auto(
        openai_response,
        provider_type,
        request_model,
        cost_calculator,
    )
}

/// ç»Ÿä¸€çš„å“åº”è½¬æ¢å®ç°
///
/// è¿™ä¸ªå‡½æ•°è‡ªåŠ¨æ ¹æ®å“åº”æ•°æ®åˆ¤æ–­æ˜¯å¦éœ€è¦è§£æå‡½æ•°è°ƒç”¨
fn convert_response_auto(
    openai_response: OpenAiResponse,
    provider_type: AiProviderType,
    request_model: &str,
    cost_calculator: impl Fn(&str, usize, usize) -> Option<f64>,
) -> AiResult<AiResponse> {
    let choice = openai_response.choices.first().ok_or_else(|| {
        AiErrReason::Uvs(UvsReason::from_logic(
            "TODO: no choices in response".to_string(),
        ))
        .to_err()
    })?;

    // è°ƒè¯•è¾“å‡ºï¼šæ£€æŸ¥choiceä¸­çš„tool_calls
    println!(
        "ğŸ” å“åº”è½¬æ¢è°ƒè¯• - choice.tool_calls: {:?}",
        choice.tool_calls
    );
    println!(
        "ğŸ” å“åº”è½¬æ¢è°ƒè¯• - choice.message.tool_calls: {:?}",
        choice.message.tool_calls
    );

    // è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦è§£æå‡½æ•°è°ƒç”¨
    // ä¼˜å…ˆä»message.tool_callsè¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»choice.tool_callsè¯»å–
    let tool_calls = if choice.message.tool_calls.is_some() {
        choice.message.tool_calls.as_ref()
    } else {
        choice.tool_calls.as_ref()
    }
    .map(|tool_calls| {
        println!(
            "ğŸ” å“åº”è½¬æ¢è°ƒè¯• - è§£ætool_callsï¼Œæ•°é‡: {}",
            tool_calls.len()
        );
        for (i, tool_call) in tool_calls.iter().enumerate() {
            println!(
                "   - Tool Call {}: {} with args: {}",
                i, tool_call.function.name, tool_call.function.arguments
            );
        }
        tool_calls
            .iter()
            .map(|tool_call| FunctionCall {
                index: tool_call.index,
                id: tool_call.id.clone(),
                r#type: tool_call.r#type.clone(),
                function: FunctionCallInfo {
                    name: tool_call.function.name.clone(),
                    arguments: tool_call.function.arguments.clone(),
                },
            })
            .collect()
    });

    // è½¬æ¢ä½¿ç”¨ä¿¡æ¯
    let prompt_tokens = openai_response
        .usage
        .as_ref()
        .map(|u| u.prompt_tokens)
        .unwrap_or(0);

    let completion_tokens = openai_response
        .usage
        .as_ref()
        .map(|u| u.completion_tokens)
        .unwrap_or(0);

    let total_tokens = openai_response
        .usage
        .as_ref()
        .map(|u| u.total_tokens)
        .unwrap_or(0);

    let estimated_cost = cost_calculator(request_model, prompt_tokens, completion_tokens);

    Ok(AiResponse {
        content: choice.message.content.clone(),
        model: openai_response.model.clone(),
        usage: UsageInfo {
            prompt_tokens,
            completion_tokens,
            total_tokens,
            estimated_cost,
        },
        finish_reason: choice.finish_reason.clone(),
        provider: provider_type,
        metadata: std::collections::HashMap::new(),
        tool_calls,
    })
}

#[cfg(test)]
mod helper_tests {
    use super::*;

    #[test]
    fn test_convert_response_with_functions_helper_success() {
        // åˆ›å»º JSON å“åº”æ–‡æœ¬ï¼ˆå¸¦å‡½æ•°è°ƒç”¨ï¼‰
        let json_response = r#"
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "æˆ‘æ¥å¸®æ‚¨æ‰§è¡ŒGitæ“ä½œ"
            },
            "finish_reason": "tool_calls",
            "tool_calls": [
                {
                    "index": 0,
                    "id": "call_0_889decaf-c79e-4e8c-8655-fe0d7805298c",
                    "type": "function",
                    "function": {
                        "name": "git_status",
                        "arguments": "{}"
                    }
                }
            ]
        }
    ],
    "usage": {
        "prompt_tokens": 398,
        "completion_tokens": 24,
        "total_tokens": 422
    },
    "model": "deepseek-chat"
}
"#;

        // è½¬æ¢å“åº”
        let result = convert_response_from_text(
            json_response,
            AiProviderType::DeepSeek,
            "deepseek-chat",
            |_, _, _| Some(0.001),
        );

        // éªŒè¯ç»“æœ
        assert!(result.is_ok());
        let response = result.unwrap();

        // éªŒè¯åŸºæœ¬å“åº”ä¿¡æ¯
        assert_eq!(response.content, "æˆ‘æ¥å¸®æ‚¨æ‰§è¡ŒGitæ“ä½œ");
        assert_eq!(response.model, "deepseek-chat");
        assert_eq!(response.usage.prompt_tokens, 398);
        assert_eq!(response.usage.completion_tokens, 24);
        assert_eq!(response.usage.total_tokens, 422);
        assert_eq!(response.usage.estimated_cost, Some(0.001));
        assert_eq!(response.finish_reason, Some("tool_calls".to_string()));
        assert_eq!(response.provider, AiProviderType::DeepSeek);

        // éªŒè¯å‡½æ•°è°ƒç”¨
        assert!(response.tool_calls.is_some());
        let tool_calls = response.tool_calls.as_ref().unwrap();
        assert_eq!(tool_calls.len(), 1);

        let tool_call = &tool_calls[0];
        assert_eq!(tool_call.index, Some(0));
        assert_eq!(tool_call.id, "call_0_889decaf-c79e-4e8c-8655-fe0d7805298c");
        assert_eq!(tool_call.r#type, "function");
        assert_eq!(tool_call.function.name, "git_status");
        assert_eq!(tool_call.function.arguments, "{}");
    }

    #[test]
    fn test_convert_response_with_functions_helper_no_choices() {
        // åˆ›å»ºæ²¡æœ‰ choices çš„ JSON å“åº”
        let json_response = r#"
{
    "choices": [],
    "usage": null,
    "model": "test-model"
}
"#;

        let result = convert_response_from_text(
            json_response,
            AiProviderType::OpenAi,
            "test-model",
            |_, _, _| None,
        );

        // éªŒè¯è¿”å›é”™è¯¯
        assert!(result.is_err());
        let error = result.unwrap_err();
        assert_eq!(
            error.to_string(),
            "[800] BUG :logic error << \"TODO: no choices in response\""
        );
    }

    #[test]
    fn test_convert_response_helper_success() {
        // åˆ›å»º JSON å“åº”æ–‡æœ¬ï¼ˆæ— å‡½æ•°è°ƒç”¨ï¼‰
        let json_response = r#"
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”"
            },
            "finish_reason": "stop",
            "tool_calls": null
        }
    ],
    "usage": {
        "prompt_tokens": 100,
        "completion_tokens": 50,
        "total_tokens": 150
    },
    "model": "gpt-4"
}
"#;

        // è½¬æ¢å“åº”
        let result = convert_response_from_text(
            json_response,
            AiProviderType::OpenAi,
            "gpt-4",
            |_, _, _| Some(0.003),
        );

        // éªŒè¯ç»“æœ
        assert!(result.is_ok());
        let response = result.unwrap();

        // éªŒè¯å“åº”ä¿¡æ¯
        assert_eq!(response.content, "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”");
        assert_eq!(response.model, "gpt-4");
        assert_eq!(response.usage.prompt_tokens, 100);
        assert_eq!(response.usage.completion_tokens, 50);
        assert_eq!(response.usage.total_tokens, 150);
        assert_eq!(response.usage.estimated_cost, Some(0.003));
        assert_eq!(response.finish_reason, Some("stop".to_string()));
        assert_eq!(response.provider, AiProviderType::OpenAi);
        assert!(response.tool_calls.is_none());
    }

    #[test]
    fn test_convert_response_helper_no_choices() {
        // åˆ›å»ºæ²¡æœ‰ choices çš„ JSON å“åº”
        let json_response = r#"
{
    "choices": [],
    "usage": null,
    "model": "test-model"
}
"#;

        let result = convert_response_from_text(
            json_response,
            AiProviderType::OpenAi,
            "test-model",
            |_, _, _| None,
        );

        // éªŒè¯è¿”å›é”™è¯¯
        assert!(result.is_err());
        let error = result.unwrap_err();
        assert_eq!(
            error.to_string(),
            "[800] BUG :logic error << \"TODO: no choices in response\""
        );
    }

    #[test]
    fn test_convert_response_helper_no_usage() {
        // åˆ›å»ºæ²¡æœ‰ä½¿ç”¨ä¿¡æ¯çš„ JSON å“åº”
        let json_response = r#"
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "å“åº”å†…å®¹"
            },
            "finish_reason": "stop",
            "tool_calls": null
        }
    ],
    "usage": null,
    "model": "gpt-3.5-turbo"
}
"#;

        let result = convert_response_from_text(
            json_response,
            AiProviderType::OpenAi,
            "gpt-3.5-turbo",
            |_, _, _| None,
        );

        // éªŒè¯ç»“æœ
        assert!(result.is_ok());
        let response = result.unwrap();

        // éªŒè¯é»˜è®¤å€¼
        assert_eq!(response.content, "å“åº”å†…å®¹");
        assert_eq!(response.usage.prompt_tokens, 0);
        assert_eq!(response.usage.completion_tokens, 0);
        assert_eq!(response.usage.total_tokens, 0);
        assert_eq!(response.usage.estimated_cost, None);
        assert_eq!(response.finish_reason, Some("stop".to_string()));
        assert!(response.tool_calls.is_none());
    }

    #[test]
    fn test_convert_response_with_functions_helper_multiple_tool_calls() {
        // åˆ›å»ºå¤šä¸ªå‡½æ•°è°ƒç”¨çš„ JSON å“åº”
        let json_response = r#"
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "æ‰§è¡Œå®Œæ•´çš„Gitå·¥ä½œæµ"
            },
            "finish_reason": "tool_calls",
            "tool_calls": [
                {
                    "index": 0,
                    "id": "call_001",
                    "type": "function",
                    "function": {
                        "name": "git_status",
                        "arguments": "{}"
                    }
                },
                {
                    "index": 1,
                    "id": "call_002",
                    "type": "function",
                    "function": {
                        "name": "git_add",
                        "arguments": "{\"files\": [\".\"]}"
                    }
                },
                {
                    "index": 2,
                    "id": "call_003",
                    "type": "function",
                    "function": {
                        "name": "git_commit",
                        "arguments": "{\"message\": \"Test commit\"}"
                    }
                }
            ]
        }
    ],
    "usage": {
        "prompt_tokens": 500,
        "completion_tokens": 100,
        "total_tokens": 600
    },
    "model": "gpt-4-turbo"
}
"#;

        let result = convert_response_from_text(
            json_response,
            AiProviderType::OpenAi,
            "gpt-4-turbo",
            |_, _, _| Some(0.006),
        );

        // éªŒè¯ç»“æœ
        assert!(result.is_ok());
        let response = result.unwrap();

        // éªŒè¯å¤šä¸ªå‡½æ•°è°ƒç”¨
        assert!(response.tool_calls.is_some());
        let tool_calls = response.tool_calls.as_ref().unwrap();
        assert_eq!(tool_calls.len(), 3);

        // éªŒè¯ç¬¬ä¸€ä¸ªè°ƒç”¨
        assert_eq!(tool_calls[0].function.name, "git_status");
        assert_eq!(tool_calls[0].function.arguments, "{}");

        // éªŒè¯ç¬¬äºŒä¸ªè°ƒç”¨
        assert_eq!(tool_calls[1].function.name, "git_add");
        assert_eq!(tool_calls[1].function.arguments, "{\"files\": [\".\"]}");

        // éªŒè¯ç¬¬ä¸‰ä¸ªè°ƒç”¨
        assert_eq!(tool_calls[2].function.name, "git_commit");
        assert_eq!(
            tool_calls[2].function.arguments,
            "{\"message\": \"Test commit\"}"
        );
    }
    #[test]
    fn test_convert_response_from_text() {
        // åˆ›å»ºå¸¦æœ‰å‡½æ•°è°ƒç”¨çš„ JSON å“åº”æ–‡æœ¬
        let json_response_with_tool_calls = r#"
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "æˆ‘æ¥å¸®æ‚¨æ‰§è¡ŒGitæ“ä½œ"
            },
            "finish_reason": "tool_calls",
            "tool_calls": [
                {
                    "index": 0,
                    "id": "call_0_889decaf-c79e-4e8c-8655-fe0d7805298c",
                    "type": "function",
                    "function": {
                        "name": "git_status",
                        "arguments": "{}"
                    }
                }
            ]
        }
    ],
    "usage": {
        "prompt_tokens": 398,
        "completion_tokens": 24,
        "total_tokens": 422
    },
    "model": "deepseek-chat"
}
"#;

        // åˆ›å»ºä¸å¸¦æœ‰å‡½æ•°è°ƒç”¨çš„ JSON å“åº”æ–‡æœ¬
        let json_response_without_tool_calls = r#"
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”"
            },
            "finish_reason": "stop",
            "tool_calls": null
        }
    ],
    "usage": {
        "prompt_tokens": 100,
        "completion_tokens": 50,
        "total_tokens": 150
    },
    "model": "gpt-4"
}
"#;

        // æµ‹è¯•å¸¦å‡½æ•°è°ƒç”¨çš„å“åº”è½¬æ¢
        let result_with_tool_calls = convert_response_from_text(
            json_response_with_tool_calls,
            AiProviderType::DeepSeek,
            "deepseek-chat",
            |_, _, _| Some(0.001),
        );

        // æµ‹è¯•ä¸å¸¦å‡½æ•°è°ƒç”¨çš„å“åº”è½¬æ¢
        let result_without_tool_calls = convert_response_from_text(
            json_response_without_tool_calls,
            AiProviderType::OpenAi,
            "gpt-4",
            |_, _, _| Some(0.003),
        );

        // éªŒè¯å¸¦å‡½æ•°è°ƒç”¨çš„å“åº”
        assert!(result_with_tool_calls.is_ok());
        let response_with_tool_calls = result_with_tool_calls.unwrap();
        assert!(response_with_tool_calls.tool_calls.is_some());
        let tool_calls = response_with_tool_calls.tool_calls.as_ref().unwrap();
        assert_eq!(tool_calls.len(), 1);
        assert_eq!(tool_calls[0].function.name, "git_status");
        assert_eq!(tool_calls[0].function.arguments, "{}");

        // éªŒè¯ä¸å¸¦å‡½æ•°è°ƒç”¨çš„å“åº”
        assert!(result_without_tool_calls.is_ok());
        let response_without_tool_calls = result_without_tool_calls.unwrap();
        assert!(response_without_tool_calls.tool_calls.is_none());
        assert_eq!(response_without_tool_calls.content, "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”");
    }

    #[test]
    fn test_convert_response_auto_detect_tool_calls() {
        // æµ‹è¯• convert_response_from_text è‡ªåŠ¨æ£€æµ‹å‡½æ•°è°ƒç”¨
        let result_with_tool_calls = convert_response_from_text(
            &create_openai_response_with_tool_calls_json(),
            AiProviderType::DeepSeek,
            "deepseek-chat",
            |_, _, _| Some(0.001),
        );

        let result_without_tool_calls = convert_response_from_text(
            &create_openai_response_without_tool_calls_json(),
            AiProviderType::OpenAi,
            "gpt-4",
            |_, _, _| Some(0.003),
        );

        // éªŒè¯æœ‰å‡½æ•°è°ƒç”¨çš„å“åº”
        assert!(result_with_tool_calls.is_ok());
        let response_with_tool_calls = result_with_tool_calls.unwrap();
        assert!(response_with_tool_calls.tool_calls.is_some());
        let tool_calls = response_with_tool_calls.tool_calls.as_ref().unwrap();
        assert_eq!(tool_calls.len(), 1);
        assert_eq!(tool_calls[0].function.name, "git_status");

        // éªŒè¯æ— å‡½æ•°è°ƒç”¨çš„å“åº”
        assert!(result_without_tool_calls.is_ok());
        let response_without_tool_calls = result_without_tool_calls.unwrap();
        assert!(response_without_tool_calls.tool_calls.is_none());
    }

    // è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºå¸¦æœ‰å‡½æ•°è°ƒç”¨çš„å“åº”JSON
    fn create_openai_response_with_tool_calls_json() -> String {
        r#"
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "æˆ‘æ¥å¸®æ‚¨æ‰§è¡ŒGitæ“ä½œ"
            },
            "finish_reason": "tool_calls",
            "tool_calls": [
                {
                    "index": 0,
                    "id": "call_0_889decaf-c79e-4e8c-8655-fe0d7805298c",
                    "type": "function",
                    "function": {
                        "name": "git_status",
                        "arguments": "{}"
                    }
                }
            ]
        }
    ],
    "usage": {
        "prompt_tokens": 398,
        "completion_tokens": 24,
        "total_tokens": 422
    },
    "model": "deepseek-chat"
}
"#
        .to_string()
    }

    // è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºä¸å¸¦æœ‰å‡½æ•°è°ƒç”¨çš„å“åº”JSON
    fn create_openai_response_without_tool_calls_json() -> String {
        r#"
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”"
            },
            "finish_reason": "stop",
            "tool_calls": null
        }
    ],
    "usage": {
        "prompt_tokens": 100,
        "completion_tokens": 50,
        "total_tokens": 150
    },
    "model": "gpt-4"
}
"#
        .to_string()
    }

    #[test]
    fn test_convert_response_from_text_invalid_json() {
        // æµ‹è¯•æ— æ•ˆ JSON çš„å¤„ç†
        let invalid_json = r#"{"invalid": json}"#;

        let result =
            convert_response_from_text(invalid_json, AiProviderType::OpenAi, "gpt-4", |_, _, _| {
                None
            });

        // éªŒè¯è¿”å›è§£æé”™è¯¯
        assert!(result.is_err());
        let error = result.unwrap_err();
        let error_msg = error.to_string();

        // æ ¹æ®å®é™…é”™è¯¯æ¶ˆæ¯æ›´æ–°æ–­è¨€
        assert!(
            error_msg.contains("data error")
                || error_msg.contains("parse error")
                || error_msg.contains("expected value")
        );
    }

    #[test]
    fn test_convert_response_from_text_empty_tool_calls() {
        // åˆ›å»ºå¸¦æœ‰ç©ºå‡½æ•°è°ƒç”¨æ•°ç»„çš„ JSON å“åº”æ–‡æœ¬
        let json_response_empty_tool_calls = r#"
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "å“åº”å†…å®¹"
            },
            "finish_reason": "tool_calls",
            "tool_calls": []
        }
    ],
    "usage": {
        "prompt_tokens": 100,
        "completion_tokens": 50,
        "total_tokens": 150
    },
    "model": "gpt-4"
}
"#;

        let result = convert_response_from_text(
            json_response_empty_tool_calls,
            AiProviderType::OpenAi,
            "gpt-4",
            |_, _, _| Some(0.003),
        );

        // éªŒè¯ç©ºå‡½æ•°è°ƒç”¨æ•°ç»„çš„å¤„ç†
        assert!(result.is_ok());
        let response = result.unwrap();
        assert!(response.tool_calls.is_some());
        assert_eq!(response.tool_calls.as_ref().unwrap().len(), 0);
    }

    // è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºå¸¦æœ‰ç©ºå‡½æ•°è°ƒç”¨æ•°ç»„çš„å“åº”JSON
    fn create_openai_response_with_empty_tool_calls_json() -> String {
        r#"
{
    "choices": [
        {
            "message": {
                "role": "assistant",
                "content": "å“åº”å†…å®¹"
            },
            "finish_reason": "tool_calls",
            "tool_calls": []
        }
    ],
    "usage": {
        "prompt_tokens": 100,
        "completion_tokens": 50,
        "total_tokens": 150
    },
    "model": "gpt-4"
}
"#
        .to_string()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::providers::openai::{Choice, Message, OpenAiFunctionCall, OpenAiToolCall, Usage};

    #[test]
    fn test_convert_response_without_functions() {
        let converter = OpenAiResponseConverter::new(AiProviderType::OpenAi);

        // åˆ›å»ºæ¨¡æ‹Ÿçš„ OpenAI å“åº”
        let openai_response = OpenAiResponse {
            choices: vec![Choice {
                message: Message {
                    role: "assistant".to_string(),
                    content: "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”".to_string(),
                },
                finish_reason: Some("stop".to_string()),
                tool_calls: None,
            }],
            usage: Some(Usage {
                prompt_tokens: 100,
                completion_tokens: 50,
                total_tokens: 150,
            }),
            model: "gpt-4".to_string(),
        };

        // è½¬æ¢å“åº”
        let response = converter.convert_response(openai_response, "gpt-4", |_, _, _| Some(0.003));

        // éªŒè¯ç»“æœ
        assert_eq!(response.content, "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”");
        assert_eq!(response.model, "gpt-4");
        assert_eq!(response.usage.prompt_tokens, 100);
        assert_eq!(response.usage.completion_tokens, 50);
        assert_eq!(response.usage.total_tokens, 150);
        assert_eq!(response.usage.estimated_cost, Some(0.003));
        assert_eq!(response.finish_reason, Some("stop".to_string()));
        assert_eq!(response.provider, AiProviderType::OpenAi);
        assert!(response.tool_calls.is_none());
    }

    #[test]
    fn test_convert_response_with_functions() {
        let converter = OpenAiResponseConverter::new(AiProviderType::DeepSeek);

        // åˆ›å»ºæ¨¡æ‹Ÿçš„ OpenAI å“åº”ï¼ˆå¸¦å‡½æ•°è°ƒç”¨ï¼‰
        let openai_response = OpenAiResponse {
            choices: vec![Choice {
                message: Message {
                    role: "assistant".to_string(),
                    content: "æˆ‘æ¥å¸®æ‚¨æ‰§è¡ŒGitæ“ä½œ".to_string(),
                },
                finish_reason: Some("tool_calls".to_string()),
                tool_calls: Some(vec![OpenAiToolCall {
                    index: Some(0),
                    id: "call_0_889decaf-c79e-4e8c-8655-fe0d7805298c".to_string(),
                    r#type: "function".to_string(),
                    function: OpenAiFunctionCall {
                        name: "git_status".to_string(),
                        arguments: "{}".to_string(),
                    },
                }]),
            }],
            usage: Some(Usage {
                prompt_tokens: 398,
                completion_tokens: 24,
                total_tokens: 422,
            }),
            model: "deepseek-chat".to_string(),
        };

        // è½¬æ¢å“åº”
        let response = converter.convert_response_with_functions(
            openai_response,
            "deepseek-chat",
            |_, _, _| Some(0.001),
        );

        // éªŒè¯åŸºæœ¬å“åº”ä¿¡æ¯
        assert_eq!(response.content, "æˆ‘æ¥å¸®æ‚¨æ‰§è¡ŒGitæ“ä½œ");
        assert_eq!(response.model, "deepseek-chat");
        assert_eq!(response.usage.prompt_tokens, 398);
        assert_eq!(response.usage.completion_tokens, 24);
        assert_eq!(response.usage.total_tokens, 422);
        assert_eq!(response.usage.estimated_cost, Some(0.001));
        assert_eq!(response.finish_reason, Some("tool_calls".to_string()));
        assert_eq!(response.provider, AiProviderType::DeepSeek);

        // éªŒè¯å‡½æ•°è°ƒç”¨
        assert!(response.tool_calls.is_some());
        let tool_calls = response.tool_calls.as_ref().unwrap();
        assert_eq!(tool_calls.len(), 1);

        let tool_call = &tool_calls[0];
        assert_eq!(tool_call.index, Some(0));
        assert_eq!(tool_call.id, "call_0_889decaf-c79e-4e8c-8655-fe0d7805298c");
        assert_eq!(tool_call.r#type, "function");
        assert_eq!(tool_call.function.name, "git_status");
        assert_eq!(tool_call.function.arguments, "{}");
    }

    #[test]
    fn test_convert_response_no_usage() {
        let converter = OpenAiResponseConverter::new(AiProviderType::OpenAi);

        // åˆ›å»ºæ²¡æœ‰ä½¿ç”¨ä¿¡æ¯çš„å“åº”
        let openai_response = OpenAiResponse {
            choices: vec![Choice {
                message: Message {
                    role: "assistant".to_string(),
                    content: "å“åº”å†…å®¹".to_string(),
                },
                finish_reason: Some("stop".to_string()),
                tool_calls: None,
            }],
            usage: None,
            model: "gpt-3.5-turbo".to_string(),
        };

        // è½¬æ¢å“åº”ï¼ˆä½¿ç”¨ä¸è®¡ç®—æˆæœ¬çš„å‡½æ•°ï¼‰
        let response = converter.convert_response(openai_response, "gpt-3.5-turbo", |_, _, _| None);

        // éªŒè¯é»˜è®¤å€¼
        assert_eq!(response.content, "å“åº”å†…å®¹");
        assert_eq!(response.usage.prompt_tokens, 0);
        assert_eq!(response.usage.completion_tokens, 0);
        assert_eq!(response.usage.total_tokens, 0);
        assert_eq!(response.usage.estimated_cost, None);
        assert_eq!(response.finish_reason, Some("stop".to_string()));
        assert!(response.tool_calls.is_none());
    }

    #[test]
    fn test_convert_response_multiple_tool_calls() {
        let converter = OpenAiResponseConverter::new(AiProviderType::OpenAi);

        // åˆ›å»ºå¤šä¸ªå‡½æ•°è°ƒç”¨çš„å“åº”
        let openai_response = OpenAiResponse {
            choices: vec![Choice {
                message: Message {
                    role: "assistant".to_string(),
                    content: "æ‰§è¡Œå®Œæ•´çš„Gitå·¥ä½œæµ".to_string(),
                },
                finish_reason: Some("tool_calls".to_string()),
                tool_calls: Some(vec![
                    OpenAiToolCall {
                        index: Some(0),
                        id: "call_001".to_string(),
                        r#type: "function".to_string(),
                        function: OpenAiFunctionCall {
                            name: "git_status".to_string(),
                            arguments: "{}".to_string(),
                        },
                    },
                    OpenAiToolCall {
                        index: Some(1),
                        id: "call_002".to_string(),
                        r#type: "function".to_string(),
                        function: OpenAiFunctionCall {
                            name: "git_add".to_string(),
                            arguments: "{\"files\": [\".\"]}".to_string(),
                        },
                    },
                    OpenAiToolCall {
                        index: Some(2),
                        id: "call_003".to_string(),
                        r#type: "function".to_string(),
                        function: OpenAiFunctionCall {
                            name: "git_commit".to_string(),
                            arguments: "{\"message\": \"Test commit\"}".to_string(),
                        },
                    },
                ]),
            }],
            usage: Some(Usage {
                prompt_tokens: 500,
                completion_tokens: 100,
                total_tokens: 600,
            }),
            model: "gpt-4-turbo".to_string(),
        };

        let response =
            converter.convert_response_with_functions(openai_response, "gpt-4-turbo", |_, _, _| {
                Some(0.006)
            });

        // éªŒè¯å¤šä¸ªå‡½æ•°è°ƒç”¨
        assert!(response.tool_calls.is_some());
        let tool_calls = response.tool_calls.as_ref().unwrap();
        assert_eq!(tool_calls.len(), 3);

        // éªŒè¯ç¬¬ä¸€ä¸ªè°ƒç”¨
        assert_eq!(tool_calls[0].function.name, "git_status");
        assert_eq!(tool_calls[0].function.arguments, "{}");

        // éªŒè¯ç¬¬äºŒä¸ªè°ƒç”¨
        assert_eq!(tool_calls[1].function.name, "git_add");
        assert_eq!(tool_calls[1].function.arguments, "{\"files\": [\".\"]}");

        // éªŒè¯ç¬¬ä¸‰ä¸ªè°ƒç”¨
        assert_eq!(tool_calls[2].function.name, "git_commit");
        assert_eq!(
            tool_calls[2].function.arguments,
            "{\"message\": \"Test commit\"}"
        );
    }

    #[test]
    fn test_convert_response_empty_tool_calls() {
        let converter = OpenAiResponseConverter::new(AiProviderType::OpenAi);

        // åˆ›å»ºç©ºçš„å·¥å…·è°ƒç”¨æ•°ç»„
        let openai_response = OpenAiResponse {
            choices: vec![Choice {
                message: Message {
                    role: "assistant".to_string(),
                    content: "å“åº”å†…å®¹".to_string(),
                },
                finish_reason: Some("tool_calls".to_string()),
                tool_calls: Some(vec![]), // ç©ºæ•°ç»„
            }],
            usage: Some(Usage {
                prompt_tokens: 100,
                completion_tokens: 50,
                total_tokens: 150,
            }),
            model: "gpt-4".to_string(),
        };

        let response =
            converter
                .convert_response_with_functions(openai_response, "gpt-4", |_, _, _| Some(0.003));

        // ç©ºçš„å·¥å…·è°ƒç”¨æ•°ç»„åº”è¯¥ä¿æŒä¸ºç©ºæ•°ç»„
        assert!(response.tool_calls.is_some());
        let tool_calls = response.tool_calls.as_ref().unwrap();
        assert_eq!(tool_calls.len(), 0);
    }
}
